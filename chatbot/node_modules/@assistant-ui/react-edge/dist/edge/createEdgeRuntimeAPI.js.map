{"version":3,"sources":["../../src/edge/createEdgeRuntimeAPI.ts"],"sourcesContent":["import {\n  LanguageModelV1,\n  LanguageModelV1ToolChoice,\n  LanguageModelV1FunctionTool,\n  LanguageModelV1Prompt,\n  LanguageModelV1CallOptions,\n} from \"@ai-sdk/provider\";\nimport { EdgeRuntimeRequestOptionsSchema } from \"./EdgeRuntimeRequestOptions\";\nimport { toLanguageModelMessages } from \"../converters/toLanguageModelMessages\";\nimport { toLanguageModelTools } from \"../converters/toLanguageModelTools\";\nimport { z } from \"zod\";\nimport {\n  AssistantMessage,\n  AssistantMessageAccumulator,\n  AssistantStreamChunk,\n  DataStreamEncoder,\n  unstable_toolResultStream,\n} from \"assistant-stream\";\nimport { LanguageModelV1StreamDecoder } from \"assistant-stream/ai-sdk\";\nimport { ThreadMessage, Tool } from \"@assistant-ui/react\";\nimport { CoreMessage } from \"./CoreTypes\";\nimport {\n  LanguageModelConfigSchema,\n  LanguageModelV1CallSettingsSchema,\n} from \"./schemas\";\n\nexport type LanguageModelV1CallSettings = z.infer<\n  typeof LanguageModelV1CallSettingsSchema\n>;\nexport type LanguageModelConfig = z.infer<typeof LanguageModelConfigSchema>;\n\ntype LanguageModelCreator = (\n  config: LanguageModelConfig,\n) => Promise<LanguageModelV1> | LanguageModelV1;\n\ntype ThreadStep = {\n  readonly messageId?: string;\n  readonly usage?:\n    | {\n        readonly promptTokens: number;\n        readonly completionTokens: number;\n      }\n    | undefined;\n};\n\ntype FinishResult = {\n  messages: readonly (CoreMessage | ThreadMessage)[];\n  metadata: {\n    steps: readonly ThreadStep[];\n  };\n};\n\nexport type CreateEdgeRuntimeAPIOptions = LanguageModelV1CallSettings & {\n  model: LanguageModelV1 | LanguageModelCreator;\n  system?: string;\n  tools?: Record<string, Tool<any, any>>;\n  toolChoice?: LanguageModelV1ToolChoice;\n  onFinish?: (result: FinishResult) => void;\n};\n\ntype GetEdgeRuntimeStreamOptions = {\n  abortSignal: AbortSignal;\n  requestData: z.infer<typeof EdgeRuntimeRequestOptionsSchema>;\n  options: CreateEdgeRuntimeAPIOptions;\n};\n\nexport const getEdgeRuntimeStream = async ({\n  abortSignal,\n  requestData: unsafeRequest,\n  options: {\n    model: modelOrCreator,\n    system: serverSystem,\n    tools: serverTools = {},\n    toolChoice,\n    onFinish,\n    ...unsafeSettings\n  },\n}: GetEdgeRuntimeStreamOptions) => {\n  const settings = LanguageModelV1CallSettingsSchema.parse(unsafeSettings);\n  const lmServerTools = toLanguageModelTools(serverTools);\n  const hasServerTools = Object.values(serverTools).some((v) => !!v.execute);\n\n  const {\n    system: clientSystem,\n    tools: clientTools = [],\n    messages,\n    apiKey,\n    baseUrl,\n    modelName,\n    ...callSettings\n  } = EdgeRuntimeRequestOptionsSchema.parse(unsafeRequest);\n\n  const systemMessages = [];\n  if (serverSystem) systemMessages.push(serverSystem);\n  if (clientSystem) systemMessages.push(clientSystem);\n  const system = systemMessages.join(\"\\n\\n\");\n\n  for (const clientTool of clientTools) {\n    if (serverTools?.[clientTool.name]) {\n      throw new Error(\n        `Tool ${clientTool.name} was defined in both the client and server tools. This is not allowed.`,\n      );\n    }\n  }\n\n  const model =\n    typeof modelOrCreator === \"function\"\n      ? await modelOrCreator({ apiKey, baseUrl, modelName })\n      : modelOrCreator;\n\n  let stream: ReadableStream<AssistantStreamChunk>;\n  const streamResult = await streamMessage({\n    ...(settings as Partial<StreamMessageOptions>),\n    ...callSettings,\n\n    model,\n    abortSignal,\n\n    ...(!!system ? { system } : undefined),\n    messages,\n    tools: lmServerTools.concat(clientTools as LanguageModelV1FunctionTool[]),\n    ...(toolChoice ? { toolChoice } : undefined),\n  });\n  stream = streamResult.stream.pipeThrough(new LanguageModelV1StreamDecoder());\n\n  // add tool results if we have server tools\n  const canExecuteTools = hasServerTools && toolChoice?.type !== \"none\";\n  if (canExecuteTools) {\n    stream = stream.pipeThrough(\n      unstable_toolResultStream(serverTools, abortSignal),\n    );\n  }\n\n  if (canExecuteTools || onFinish) {\n    // tee the stream to process server tools and onFinish asap\n    const tees = stream.tee();\n    stream = tees[0];\n    let serverStream = tees[1];\n\n    if (onFinish) {\n      let lastChunk: AssistantMessage | undefined;\n      serverStream.pipeThrough(new AssistantMessageAccumulator()).pipeTo(\n        new WritableStream({\n          write(chunk) {\n            lastChunk = chunk;\n          },\n          close() {\n            if (!lastChunk?.status || lastChunk.status.type === \"running\")\n              return;\n\n            const resultingMessages = [\n              ...messages,\n              {\n                id: \"DEFAULT\",\n                createdAt: new Date(),\n                role: \"assistant\",\n                content: lastChunk.content,\n                status: lastChunk.status,\n                metadata: lastChunk.metadata,\n              } satisfies ThreadMessage,\n            ];\n            onFinish({\n              messages: resultingMessages,\n              metadata: {\n                steps: lastChunk.metadata.steps,\n              },\n            });\n          },\n          abort(e) {\n            console.error(\"Server stream processing error:\", e);\n          },\n        }),\n      );\n    }\n  }\n\n  return stream;\n};\n\nexport declare namespace getEdgeRuntimeResponse {\n  export type { GetEdgeRuntimeStreamOptions as Options };\n}\n\nexport const getEdgeRuntimeResponse = async (\n  options: getEdgeRuntimeResponse.Options,\n) => {\n  const stream = await getEdgeRuntimeStream(options);\n  return new Response(stream.pipeThrough(new DataStreamEncoder()), {\n    headers: {\n      \"Content-Type\": \"text/plain; charset=utf-8\",\n      \"x-vercel-ai-data-stream\": \"v1\",\n    },\n  });\n};\n\nexport const createEdgeRuntimeAPI = (options: CreateEdgeRuntimeAPIOptions) => ({\n  POST: async (request: Request) =>\n    getEdgeRuntimeResponse({\n      abortSignal: request.signal,\n      requestData: await request.json(),\n      options,\n    }),\n});\n\ntype StreamMessageOptions = LanguageModelV1CallSettings & {\n  model: LanguageModelV1;\n  system?: string;\n  messages: readonly CoreMessage[];\n  tools?: LanguageModelV1FunctionTool[];\n  toolChoice?: LanguageModelV1ToolChoice;\n  abortSignal: AbortSignal;\n};\n\nasync function streamMessage({\n  model,\n  system,\n  messages,\n  tools,\n  toolChoice,\n  ...options\n}: StreamMessageOptions) {\n  return model.doStream({\n    inputFormat: \"messages\",\n    mode: {\n      type: \"regular\",\n      ...(tools ? { tools } : undefined),\n      ...(toolChoice ? { toolChoice } : undefined),\n    },\n    prompt: convertToLanguageModelPrompt(system, messages),\n    ...(options as Partial<LanguageModelV1CallOptions>),\n  });\n}\n\nexport function convertToLanguageModelPrompt(\n  system: string | undefined,\n  messages: readonly CoreMessage[],\n): LanguageModelV1Prompt {\n  const languageModelMessages: LanguageModelV1Prompt = [];\n\n  if (system != null) {\n    languageModelMessages.push({ role: \"system\", content: system });\n  }\n  languageModelMessages.push(...toLanguageModelMessages(messages));\n\n  return languageModelMessages;\n}\n"],"mappings":";AAOA,SAAS,uCAAuC;AAChD,SAAS,+BAA+B;AACxC,SAAS,4BAA4B;AAErC;AAAA,EAEE;AAAA,EAEA;AAAA,EACA;AAAA,OACK;AACP,SAAS,oCAAoC;AAG7C;AAAA,EAEE;AAAA,OACK;AA0CA,IAAM,uBAAuB,OAAO;AAAA,EACzC;AAAA,EACA,aAAa;AAAA,EACb,SAAS;AAAA,IACP,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,OAAO,cAAc,CAAC;AAAA,IACtB;AAAA,IACA;AAAA,IACA,GAAG;AAAA,EACL;AACF,MAAmC;AACjC,QAAM,WAAW,kCAAkC,MAAM,cAAc;AACvE,QAAM,gBAAgB,qBAAqB,WAAW;AACtD,QAAM,iBAAiB,OAAO,OAAO,WAAW,EAAE,KAAK,CAAC,MAAM,CAAC,CAAC,EAAE,OAAO;AAEzE,QAAM;AAAA,IACJ,QAAQ;AAAA,IACR,OAAO,cAAc,CAAC;AAAA,IACtB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,GAAG;AAAA,EACL,IAAI,gCAAgC,MAAM,aAAa;AAEvD,QAAM,iBAAiB,CAAC;AACxB,MAAI,aAAc,gBAAe,KAAK,YAAY;AAClD,MAAI,aAAc,gBAAe,KAAK,YAAY;AAClD,QAAM,SAAS,eAAe,KAAK,MAAM;AAEzC,aAAW,cAAc,aAAa;AACpC,QAAI,cAAc,WAAW,IAAI,GAAG;AAClC,YAAM,IAAI;AAAA,QACR,QAAQ,WAAW,IAAI;AAAA,MACzB;AAAA,IACF;AAAA,EACF;AAEA,QAAM,QACJ,OAAO,mBAAmB,aACtB,MAAM,eAAe,EAAE,QAAQ,SAAS,UAAU,CAAC,IACnD;AAEN,MAAI;AACJ,QAAM,eAAe,MAAM,cAAc;AAAA,IACvC,GAAI;AAAA,IACJ,GAAG;AAAA,IAEH;AAAA,IACA;AAAA,IAEA,GAAI,CAAC,CAAC,SAAS,EAAE,OAAO,IAAI;AAAA,IAC5B;AAAA,IACA,OAAO,cAAc,OAAO,WAA4C;AAAA,IACxE,GAAI,aAAa,EAAE,WAAW,IAAI;AAAA,EACpC,CAAC;AACD,WAAS,aAAa,OAAO,YAAY,IAAI,6BAA6B,CAAC;AAG3E,QAAM,kBAAkB,kBAAkB,YAAY,SAAS;AAC/D,MAAI,iBAAiB;AACnB,aAAS,OAAO;AAAA,MACd,0BAA0B,aAAa,WAAW;AAAA,IACpD;AAAA,EACF;AAEA,MAAI,mBAAmB,UAAU;AAE/B,UAAM,OAAO,OAAO,IAAI;AACxB,aAAS,KAAK,CAAC;AACf,QAAI,eAAe,KAAK,CAAC;AAEzB,QAAI,UAAU;AACZ,UAAI;AACJ,mBAAa,YAAY,IAAI,4BAA4B,CAAC,EAAE;AAAA,QAC1D,IAAI,eAAe;AAAA,UACjB,MAAM,OAAO;AACX,wBAAY;AAAA,UACd;AAAA,UACA,QAAQ;AACN,gBAAI,CAAC,WAAW,UAAU,UAAU,OAAO,SAAS;AAClD;AAEF,kBAAM,oBAAoB;AAAA,cACxB,GAAG;AAAA,cACH;AAAA,gBACE,IAAI;AAAA,gBACJ,WAAW,oBAAI,KAAK;AAAA,gBACpB,MAAM;AAAA,gBACN,SAAS,UAAU;AAAA,gBACnB,QAAQ,UAAU;AAAA,gBAClB,UAAU,UAAU;AAAA,cACtB;AAAA,YACF;AACA,qBAAS;AAAA,cACP,UAAU;AAAA,cACV,UAAU;AAAA,gBACR,OAAO,UAAU,SAAS;AAAA,cAC5B;AAAA,YACF,CAAC;AAAA,UACH;AAAA,UACA,MAAM,GAAG;AACP,oBAAQ,MAAM,mCAAmC,CAAC;AAAA,UACpD;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAMO,IAAM,yBAAyB,OACpC,YACG;AACH,QAAM,SAAS,MAAM,qBAAqB,OAAO;AACjD,SAAO,IAAI,SAAS,OAAO,YAAY,IAAI,kBAAkB,CAAC,GAAG;AAAA,IAC/D,SAAS;AAAA,MACP,gBAAgB;AAAA,MAChB,2BAA2B;AAAA,IAC7B;AAAA,EACF,CAAC;AACH;AAEO,IAAM,uBAAuB,CAAC,aAA0C;AAAA,EAC7E,MAAM,OAAO,YACX,uBAAuB;AAAA,IACrB,aAAa,QAAQ;AAAA,IACrB,aAAa,MAAM,QAAQ,KAAK;AAAA,IAChC;AAAA,EACF,CAAC;AACL;AAWA,eAAe,cAAc;AAAA,EAC3B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAAyB;AACvB,SAAO,MAAM,SAAS;AAAA,IACpB,aAAa;AAAA,IACb,MAAM;AAAA,MACJ,MAAM;AAAA,MACN,GAAI,QAAQ,EAAE,MAAM,IAAI;AAAA,MACxB,GAAI,aAAa,EAAE,WAAW,IAAI;AAAA,IACpC;AAAA,IACA,QAAQ,6BAA6B,QAAQ,QAAQ;AAAA,IACrD,GAAI;AAAA,EACN,CAAC;AACH;AAEO,SAAS,6BACd,QACA,UACuB;AACvB,QAAM,wBAA+C,CAAC;AAEtD,MAAI,UAAU,MAAM;AAClB,0BAAsB,KAAK,EAAE,MAAM,UAAU,SAAS,OAAO,CAAC;AAAA,EAChE;AACA,wBAAsB,KAAK,GAAG,wBAAwB,QAAQ,CAAC;AAE/D,SAAO;AACT;","names":[]}